# Codeforces Multilabel Classification 

**Auteur :** Kikia Dia  
**Illuin Technology Challenge :** Data Science

---

## Contexte

Codeforces est une plateforme de programmation comp√©titive regroupant des milliers de probl√®mes d'algorithmique, chacun annot√© par plusieurs **tags** repr√©sentant les notions algorithmiques mobilis√©es (`math`, `graphs`, `strings`, etc.).

Ce projet s'appuie sur un **sous-ensemble du dataset xCodeEval** compos√© de **4 982 probl√®mes distincts**, incluant :
- `prob_desc_description` : descriptions textuelles compl√®tes ,
- `prob_desc_input_spec`, `prob_desc_output_spec` : sp√©cifications d'entr√©e/sortie,
- `prob_desc_notes` : notes √©ventuelles,
- `source_code` : solutions valid√©es en Python,
- `tags` : annotations multi-labels.

## Objectif

Construire un **algorithme de classification multi-label** capable de pr√©dire automatiquement les tags associ√©s √† un probl√®me d'algorithmique.

L'√©tude se concentre sur les **8 tags suivants** :
```python
['math', 'graphs', 'strings', 'number theory',
 'trees', 'geometry', 'games', 'probabilities']
```
Apr√®s filtrage pour ne garder que les exemples correspondant √† ces tags, le dataset final contient **2 678 probl√®mes**.

### Dataset preview

| prob_desc_description | prob_desc_input_spec | prob_desc_output_spec | prob_desc_notes | source_code | tags |
|--------------------|------------------|-------------------|---------------|-------------------|------|
| Once upon a time, Oolimry saw a suffix array... | The first line contain 2 integers $$$n$$$ and ... | Print how many strings produce such a suffix array... | NoteIn the first test case, "abb" is the only ... | import sys\ninput = sys.stdin.readline\n... | [math] |
| You are given an array of n elements, you must... | The first line contains integer n (1‚Äâ‚â§‚Äân‚Äâ‚â§‚Äâ100...) | Print integer k on the first line ‚Äî the least ... |  | def gcd(a,b):\n if b==0:return a\n return ... | [number theory, math] |

### Data Splitting

Le dataset a √©t√© divis√© en deux ensembles :
- **Train :** 80% des donn√©es (2 142 probl√®mes)
- **Validation :** 20% des donn√©es (536 probl√®mes)

---

### Structure du projet
```
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ EDA.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ Machine_learning_models.ipynb
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.py
‚îÇ   ‚îú‚îÄ‚îÄ models.py
‚îÇ   ‚îú‚îÄ‚îÄ training.py
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îî‚îÄ‚îÄ code_features.py
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ model.joblib
‚îÇ   ‚îî‚îÄ‚îÄ model_tfidf.joblib
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ requirements.txt
```
### Exploratory Data Analysis (EDA)
- Longueur des descriptions, Distribution des tags, Co-occurrence des labels, Wordclouds par tag, Analyse des patterns algorithmiques dans le code
#### Text Preprocessing

**Champs concaten√©s  pour la description :** `prob_desc_description`,`prob_desc_input_spec`,`prob_desc_output_spec`, `prob_desc_notes`

**Nettoyage du texte :** Tokenisation (NLTK), Normalisation, Suppression de stopwords, Lemmatisation
#### Wordcloud avant preprocessing

![Wordcloud avant preprocessing](images/before_preprocessing.png)

#### Wordcloud apr√®s preprocessing

![Wordcloud apr√®s preprocessing](images/after_preprocessing.png)

### Repr√©sentation des labels :
- one-hot-encoding avec `MultiLabelBinarizer`

### Text Vectorisation
**TF-IDF:**  `max_features = 5000`, `ngram_range = (1, 2)`

## Mod√©lisation

- **Strat√©gies multi-label :** `One-vs-Rest`, `MultiOutputClassifier`, `Classifier Chains`
- **Classificateurs test√©s :** `Logistic Regression`,`Random Forest`,`LinearSVC`
- **M√©triques d'√©valuation :** Micro F1-score, Macro F1-score, Hamming Loss, Subset Accuracy, Precision / Recall par tag

-**Best Model**: `OneVsRest + LinearSVC (class_weight="balanced")`, Optimisation via `GridSearchCV` 

### Approches test√©es :

- description

- description + features extraits du code Python 

- gestion du d√©s√©quilibre avec MLSMOTE (Multi-Label SMOTE)

## Evaluation

### Comparaison des approches (tri√©es par Macro F1)

| Approach | F1 Micro | F1 Macro | Hamming Loss | Subset Accuracy |
|----------|----------|----------|--------------|----------------|
| **Descriptions + code features** | **0.7251** | **0.6842** | **0.0945** | **0.4813** |
| Descriptions only | 0.7265 | 0.6653 | 0.0910 | 0.4832 |
| Resampled (SMOTE) | 0.6980 | 0.6431 | 0.0989 | 0.4627 |

**üèÜ Meilleur approche :** Descriptions + code features

### Matrices de confusion

![Matrices de confusion](images/matrices_confusion.png)

#### f1_score par tag

![f1_score_per_tag](images/f1_score.png)

**best tags ** : `math`, `strings`, `games`, `trees`

**Pour plus de d√©tails, consultez le notebook d'entra√Ænement** `notebooks/Machine_learning_models.ipynb`
---

## Utilisation du CLI

```bash
# training and evaluation on description only

python src/main.py train --data data/train --output models/model.joblib

python src/main.py evaluate --model models/model.joblib --data data/test  

# training and evaluation on description + code features 

python src/main.py train --data data/train --output models/model_hybrid.joblib --hybrid

python src/main.py evaluate --model models/model_hybrid.joblib --data data/test --hybrid --results-path results/predictions_test_hybrid.csv

```

---

## Autres Pistes d'am√©lioration

### Mod√®les Transformers

Des exp√©rimentations ont √©t√© men√©es avec des mod√®les Transformers pr√©-entra√Æn√©s, notamment :
```python
model_names = [
    "microsoft/codebert-base",
    "microsoft/graphcodebert-base",
    "microsoft/unixcoder-base",
    "bert-base-uncased"
]
```

Ces mod√®les, sp√©cialement con√ßus pour la compr√©hension du code (CodeBERT, GraphCodeBERT, UniXcoder) ou du texte naturel (BERT), pourraient potentiellement am√©liorer les performances en capturant des repr√©sentations s√©mantiques plus riches.

**Pour plus de d√©tails sur les exp√©rimentations avec les Transformers, consultez le notebook :** `notebooks/transformers.ipynb`

**Autres pistes** : 

- Fine-tuning des mod√®les Transformers sur le dataset Codeforces
- data augmentation 
